{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../KG-FPQ-data/art_YN.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "T_q = []\n",
    "F_q = []\n",
    "for data in json_data:\n",
    "    T_q.append(data['TPQ'])\n",
    "    for key, value in data.items():\n",
    "        if key.startswith(\"FPQ_\"):\n",
    "            F_q.append(value)\n",
    "\n",
    "all_q = T_q + F_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model identifier from Hugging Face\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map= \"auto\", # Auto-allocates GPU(s) if available\n",
    "    torch_dtype=torch.float16 # Optional, but saves GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for q in all_q:\n",
    "    inputs = tokenizer(q, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('llama3_8b_orig.json', 'w') as file:\n",
    "    json.dump(responses, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./g_retri_gg.json', 'r') as file:\n",
    "    detect = json.load(file)\n",
    "\n",
    "responses = []\n",
    "for i in range(len(all_q)):\n",
    "    if 'Yes' in detect[i]:\n",
    "        qq = all_q[i] + \" This question contains false premise.\" \n",
    "    else:\n",
    "        qq = all_q[i]\n",
    "        \n",
    "    inputs = tokenizer(qq, return_tensors=\"pt\").to(model.device)\n",
    "    # Generate outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    responses.append(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
